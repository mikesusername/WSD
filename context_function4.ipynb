{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Function\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This document explains and builds a context function. The purpose of the context function is to determine when two nouns are likely used in a different context for a given adjective. For example, given the adjective \"hot\", and the nouns \"woman\" and \"stove\", we recognize that a different definition is likely implied for each noun. Specifically, if hot is used to describe stove, we assume hot is meant to refer to the stove's temperature. On the other hand, if hot is used to describe \"woman\", we assume hot is meant as physically attractive.\n",
    "\n",
    "The context function takes two nouns and an adjective and makes a prediction of whether the adjective is likely used in the same context for the two nouns, or in a different context. Determining context is known as \"word sense disambiguation\" (wsd). Usually, wsd involves a word to be disambiguated, along with a sentence that can help determine the context of the word. In our case, the sentence is comprised of only two words, the noun and the adjective. This makes our task more difficult or, perhaps, more ambiguous since our lack of contextual information may not be enough to determine context. For instance, if we consider the noun \"meal\" and the adjective \"hot\", it is not entirely clear whether hot refers to the temperature or spiciness of the meal. What we are looking to accomplish with this method is an \"intuition\" of the context. To gain an \"intuition\" of the context, we will use a statistical approach.\n",
    "\n",
    "The standard method for this task uses the Lesk algorithm, and is available in the NLTK package. NLTK's lesk function takes two arguments, a sentence and a word to disambiguate. The input sentence contains the word to be disambiguated as well as other words that put the word into context. In our case, we only have a two word sentence, composed of the adjective and the noun. The lesk function returns a predicted synset (the details of how the lesk algorithm works will not be covered). A context function that determines if an adjective is likely used in the same context when paired with two different nouns can be created by comparing the two outputs of the lesk function for each noun (paired with the adjective). Unfortunately, in our case, this method has been determined to be no more successful than random guessing. Furthermore, the context function using the lesk function is biased towards predicting the adjective is used in different contexts (positive prediction) for the two nouns. This happens because the function returns a particular definition, and the more definitions an adjective has, the more likely the function will return a different definition for each noun. \n",
    "\n",
    "Our method attempts to improve over the standard lesk method, by using a statistical approach and avoids the bias introduced from adjectives that have many definitions. As mentioned previously, we want to match a definition of an adjective to a noun using a statistical method. We do this by measuring how well the noun is collocated to each word in a definition. Multiple collocation scores can be used. Then the entire definition is given a total score (again, there are several ways to compute a total score, such as average, sum, etc.). The definition scores are designed to have a positive correlation with the likelyhood that that definition is the correct context.\n",
    "\n",
    "Each noun has an associated list of scores, one score for each definition. To determine if the two nouns are used in different contexts, the score lists are compared, and a \"feature score\" is derived. The feature score will be used as a feature in a machine learning model. The feature is derived so that a high value predicts different context, and a low value predicts same context (i.e. all features will be designed so that they are positively correlated with the \"different context\" prediction). As with the definition scores, there are multiple ways to derive a feature score from the two lists, and all are based on intuition. For example, if the definition score list for a particular noun has multiple non zero values, we interpret this as uncertainty in acertaining the correct definition and should lead to a low score (erring on the side of predicting same context). On the other hand, if the two score lists have only one non zero definition score, and they are for different definitions, this is a good indication that the context is different.\n",
    "\n",
    "The exact scoring techniques will be explained in more detail below. We will take a step by step approach to building our model with explanations along the way.\n",
    "\n",
    "Using this scoring method allows us to create an unsupervised method for predicting whether the adjective is used in same context or different context for the two nouns. All we need to do is find a method of combining all features into one score. Since all features are meant to be positively correlated with a different context prediction, we may combine the scores in several different ways, such as a weighted or non weighted sum, the average, the product, etc. However, since we don't know how best to weight each feature score, we will use a supervised approach to determine the weights.\n",
    "\n",
    "A labeled data set has been constructed to be used as a train set for machine learning techniques. The train set was constructed by myself, and is, admittedly biased. However, this bias is not necessarily a bad thing...it allows the user to tailor the results to his own intuition or need. The train set (pandas dataframe) has a column of samples and a column of targets. The samples are 3-tuples in the form (noun1, noun2, adjective). The target value is 1 if the adjective is considered used in different contexts for the two nouns and a 0 if the adjective is used in the same context.\n",
    "\n",
    "Once the feature set has been created for the labeled training data, a random forrest model will be trained and evaluated. Other machine learning models have been tested on the training data, but have yielded inferior results (especially in the case of linear models), or similar results and will not be reproduced in this document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) Tagger and Lemmatizer\n",
    "\n",
    "The following block of code defines the nltk lemmatizer and trains a pos tagger. The pos tagger uses the brown corpus as training data. The tagger uses several back off taggers. The tagger, t3, is a trigram trained tagger with backoff t2. t2 is a bigram trained tagger with a cutoff of 2 and has t1 as its backoff. t1 is a unigram trained tagger that has t0 as its backoff. Finally, t0 is a default tagger that tags all words as nouns (\"NN\").\n",
    "\n",
    "We need the pos tagger to pos tag the tokens in the definitions. This allows us to look up the correct token to noun collocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the lemmatizer\n",
    "wnl=nltk.WordNetLemmatizer()\n",
    "\n",
    "#Train pos tagger\n",
    "train_sents=nltk.corpus.brown.tagged_sents()\n",
    "train_sents=[s for s in train_sents]\n",
    "t0=nltk.DefaultTagger(\"NN\")\n",
    "t1=nltk.UnigramTagger(train_sents,backoff=t0)\n",
    "t2=nltk.BigramTagger(train_sents,backoff=t1,cutoff=2)\n",
    "t3=nltk.TrigramTagger(train_sents,backoff=t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sets\n",
    "\n",
    "The three data sets we need to import are the collocations dataframe, which will be used by the scoring function, a list of stopwords, and the labeled training data. The collocations and trainset are pickled pandas dataframes. The stopwords was imported from nltk. Descriptions of the data sets will be given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import collocations database\n",
    "handle=open(\"/home/mike/databases/collocations/collocates_60k_dataframe\",\"rb\")\n",
    "collocations=pickle.load(handle)\n",
    "handle.close()\n",
    "\n",
    "#import stopwords\n",
    "sw=stopwords.words(\"english\")\n",
    "\n",
    "#import trainset\n",
    "handle=open(\"/home/mike/databases/wsd/context_trainset\",\"rb\")\n",
    "trainset=pickle.load(handle)\n",
    "handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations Dataframe\n",
    "\n",
    "The collocations dataframe has words (lemmas) and its associated collocations. The part of speech (pos) is given for the lemmas, as well as the collocates. Various collocation scores are given, such as the \"mutual information score\" (MI), total frequency of occurence (in the corpus from which it was derived), log frequency, and rank order. These scores have also been normalized by subtracting the minimum (minimum in the dataset) from each value, and then dividing by the range (max-min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>lemma</th>\n",
       "      <th>lemPoS</th>\n",
       "      <th>coll</th>\n",
       "      <th>collPoS</th>\n",
       "      <th>MI</th>\n",
       "      <th>freq</th>\n",
       "      <th>[% coll &lt; node]</th>\n",
       "      <th>rankOrder</th>\n",
       "      <th>MI_normalized</th>\n",
       "      <th>freq_normalized</th>\n",
       "      <th>log_freq</th>\n",
       "      <th>normalized_log_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>year</td>\n",
       "      <td>n</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>14261</td>\n",
       "      <td>0.3864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.026486</td>\n",
       "      <td>4.154150</td>\n",
       "      <td>0.724843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>world</td>\n",
       "      <td>n</td>\n",
       "      <td>-3.30</td>\n",
       "      <td>13092</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>0.024315</td>\n",
       "      <td>4.117006</td>\n",
       "      <td>0.718362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>same</td>\n",
       "      <td>d</td>\n",
       "      <td>-3.03</td>\n",
       "      <td>11522</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>4.061528</td>\n",
       "      <td>0.708682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID lemma lemPoS   coll collPoS    MI   freq  [% coll < node]  rankOrder  \\\n",
       "0  1   the      a   year       n -3.20  14261           0.3864        1.0   \n",
       "1  1   the      a  world       n -3.30  13092           0.1297        2.0   \n",
       "2  1   the      a   same       d -3.03  11522           0.1235        3.0   \n",
       "\n",
       "   MI_normalized  freq_normalized  log_freq  normalized_log_freq  \n",
       "0       0.010634         0.026486  4.154150             0.724843  \n",
       "1       0.006967         0.024315  4.117006             0.718362  \n",
       "2       0.016868         0.021399  4.061528             0.708682  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First three entries of the collocations dataframe\n",
    "collocations[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Set\n",
    "\n",
    "The train set is a pandas dataframe with two columns. The first column contains the samples in the form of 3-tuples, where the 3-tuples have the form (noun1, noun2, adjective). The second column is the target, 1 or 0. 1 means the adjective is used in different context for the two nouns, 0 means the same context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>(story, bark, sappy)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>(skin, humor, dark)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>(water, wind, icy)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   samples  target\n",
       "5755  (story, bark, sappy)       1\n",
       "5756   (skin, humor, dark)       1\n",
       "5757    (water, wind, icy)       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Last three entries of the trainset\n",
    "trainset[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique nouns: 287\n",
      "Number of unique adjectives: 22\n",
      "Number of samples (length of dataframe): 5758\n",
      "Percent positive targets: 53.317124001389374\n"
     ]
    }
   ],
   "source": [
    "#Let's examine the target balance, dataset length, and number of unique nouns and adjectives\n",
    "unique_nouns=set([tup[0] for tup in trainset.samples]+[tup[1] for tup in trainset.samples])\n",
    "unique_adjectives=set([tup[2] for tup in trainset.samples])\n",
    "balance=len(trainset[trainset.target==1])*100/len(trainset)\n",
    "print(\"Number of unique nouns: {}\\nNumber of unique adjectives: {}\".format(len(unique_nouns),len(unique_adjectives)))\n",
    "print(\"Number of samples (length of dataframe): {}\\nPercent positive targets: {}\".format(len(trainset),balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Functions\n",
    "\n",
    "In this section, we will define and describe the functions that are needed to compute the features. \n",
    "\n",
    "### Definition Getter\n",
    "\n",
    "The first function considered is the definition getter. This function takes the adjective as an input and outputs a list of lists. The inner lists contain 2-tuples, where the first element of the tuple is a word and the second is the pos. Each inner list corresponds to a different definition of the adjective.\n",
    "\n",
    "The function is described as follows:\n",
    "\n",
    "The definitions are obtained from nltk.corpus.wordnet. If no synsets are found for the input adjective, the function returns None. On the other hand, if the list of synsets of the adjective is non empty, the function loops over each synset (definition). For each definition, the definition is pos tagged using our trained tagger. This returns a list of tagged word/pos 2-tuples. Stop words and duplicate words are filtered out. The \"cleaned\" definition list is appended to a definitions list called adjdefs.\n",
    "\n",
    "The next step is to lemmatize the words, since this is what is used in our collocations data base. The lemmatized definitions are appended to a new definitions list called adjdefslem.\n",
    "\n",
    "The next step includes synonyms of each word in each definition. The synonyms are found using wordnet's .lemma_names() method.\n",
    "\n",
    "The next step discards duplicate words (tuples) in each definition.\n",
    "\n",
    "Finally, the last step discards duplicate definitions, insuring all definitions are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definition_getter(adj):\n",
    "    adjdefs=[]\n",
    "    syns=wn.synsets(adj,pos=\"a\")\n",
    "    if len(syns)==0:\n",
    "        return None\n",
    "    else:\n",
    "        for syn in syns:\n",
    "            adef=syn.definition()\n",
    "            pos_tagged=t3.tag(nltk.word_tokenize(adef))\n",
    "            adjdefs.append(list(set([tup for tup in pos_tagged if tup[0] not in sw and tup[0].isalpha()])))\n",
    "    \n",
    "    #Include synonyms of each word\n",
    "    adjdefs2=[]\n",
    "    for definition in adjdefs:\n",
    "        deflist1=[]\n",
    "        for tup in definition:\n",
    "            w=tup[0]\n",
    "            pos1=tup[1]\n",
    "            deflist1.append((w,pos1))\n",
    "            pos2=pos1[0].lower()\n",
    "            if pos2==\"j\":\n",
    "                pos2=\"a\"\n",
    "            try:\n",
    "                for synset in wn.synsets(w,pos=pos2):\n",
    "                    lems=synset.lemma_names()\n",
    "                    if lems:\n",
    "                        for lem in lems:\n",
    "                                deflist1.append((lem,pos1))\n",
    "            except:\n",
    "                for synset in wn.synsets(w):\n",
    "                    lems=synset.lemma_names()\n",
    "                    if lems:\n",
    "                        for lem in lems:\n",
    "                                deflist1.append((lem,pos1))\n",
    "        adjdefs2.append(list(set(deflist1)))\n",
    "    \n",
    "    #Discard duplicate words per definition\n",
    "    adjdefs3=[list(set(x)) for x in adjdefs2]\n",
    "    \n",
    "    #discard duplicate definitions\n",
    "    newlist2=[]\n",
    "    for l1 in adjdefs3:\n",
    "        l2=l1.copy()\n",
    "        l2.sort()\n",
    "        newlist2.append(tuple(l2))\n",
    "    return [list(x) for x in list(set(newlist2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Getter\n",
    "\n",
    "The score getter function takes a list of definitions (the output of the definition_getter), and a noun as the two inputs. The function returns a list of scores. The nth score of the returned list corresponds to the likelyhood that the noun should be associated with the nth definition of the adjective.\n",
    "\n",
    "There are two main collocation scoring methods, the mi score and the rank order (ro) score. These scores are preliminary scores, which score word pairs, and will be subsequently used to compute a definition score. Both scores (mi and ro) are designed to have high values for highly collocated word pairs and low values for poorly collocated word pairs. The mi score is already calculated in the collocations data frame.\n",
    "\n",
    "We will derive these intermediate scores from two different subsets. The first subset will be the collocation subset that has the noun as the lemma and the adjective as the collocation, and the second subset will be the collocation subset that has the adjective as the lemma and the noun as the collocation.\n",
    "\n",
    "The ro score will be modified from the rankOrder score in the dataframe. The the highest collocation has a score of 1 and lower collocations will have higher rankOrder scores. However, we want large values for high collocations. Thus, we can compute a normalized score by subtracting rankOrder from max(rankOrder) for the particular lemma and then dividing by the range, which is max(rankOrder)-1. For example, suppose the lemma has rankOrders ranging from 1 to 180, and the collocation score in question is 1. Then ro=(180-1)/(180-1)=1, and if the collocation score was 180, then ro=(180-180)/(180-1)=0.\n",
    "\n",
    "Each definition will be assigned seven different scores (for alternate scoring methods). The function will return a list that has the same length as the number of definitions, and the list will contain a 7-tuple containing the alternate scoring methods. The definition scores are computed as follows.\n",
    "\n",
    "For each of the two collocation subsets, there will be two scores derived, the average mi score and the average ro score. These are scores are labeled mi1rel, ro1rel, mi2rel, and ro2rel. This gives us the first four definition scores. The next score will be the average of the two previously computed mi and ro scores. Specifically, we have miavg=(mi1rel+mi2rel)/2 and roavg=(ro1rel+ro2rel)/2. The last definition score will be the average of the first four definition scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_getter(definitions,noun):\n",
    "    collocate1=collocations[(collocations.lemma==noun)]\n",
    "    collocate1=collocate1[collocate1.lemPoS==\"n\"]\n",
    "    rom=collocate1.rankOrder.max()-1\n",
    "    \n",
    "    collocate2=collocations[collocations.coll==noun]\n",
    "    collocate2=collocate2[collocate2.collPoS==\"n\"]\n",
    "    rom2=collocate2.rankOrder.max()-1\n",
    "    \n",
    "    scores=[]\n",
    "    for definition in definitions:\n",
    "        l=len(definition)\n",
    "        mi1=0\n",
    "        ro1=0\n",
    "        mi2=0\n",
    "        ro2=0\n",
    "        for tup in definition:\n",
    "            #Define the word and the word's pos\n",
    "            w=tup[0]\n",
    "            wpos=tup[1][0].lower()\n",
    "            \n",
    "            #Define the collocation where the noun is the lemma and the word is the collocation\n",
    "            collocate3=collocate1[collocate1.coll==w]\n",
    "            \n",
    "            #It is possible that collocate3 has multiple entries corresponding to different parts of speech.\n",
    "            #If this is the case, we can select the propper collocation with the following if statement\n",
    "            #and redefine collocate3\n",
    "            if wpos in list(collocate3.collPoS):\n",
    "                collocate3=collocate3[collocate3.collPoS==wpos]\n",
    "            \n",
    "            #If collocate3 is not empty (which is not guaranteed), than we will want to update our mi and\n",
    "            #ro scores\n",
    "            if len(collocate3)>0:\n",
    "                mi1+=float(collocate3.MI_normalized.iloc[0])\n",
    "                ron=float(collocate3.rankOrder.iloc[0])-1\n",
    "                ro1+=(rom-ron)/rom\n",
    "    \n",
    "            #Define the collocation where the word is the lemma and the noun is the collocation\n",
    "            collocate4=collocate2[collocate2.lemma==w]\n",
    "            if len(collocate4)>0:\n",
    "                mi2+=float(collocate4.MI_normalized.iloc[0])\n",
    "                ron2=float(collocate4.rankOrder.iloc[0])-1\n",
    "                ro2+=(rom2-ron2)/rom2\n",
    "        mi1rel=mi1/l\n",
    "        mi2rel=mi2/l\n",
    "        miavg=(mi1rel+mi2rel)/2\n",
    "        ro1rel=ro1/l\n",
    "        ro2rel=ro2/l\n",
    "        roavg=(ro1rel+ro2rel)/2\n",
    "        sumscore=(mi1rel+mi2rel+ro1rel+ro2rel)/4\n",
    "        scores.append((mi1rel,mi2rel,miavg,ro1rel,ro2rel,roavg,sumscore))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Getter\n",
    "\n",
    "The following function, called feature_getter, is the function that returns a list of feature values for a sample. Recall that a sample is a noun, noun, adjective tripple. Thus, the feature function takes as inputs the two nouns, a definition list of the adjective, which can be obtained using the definition getter functions.\n",
    "\n",
    "The feature getter returns several feature values, all of which are a measure of how likely the nouns are used in a different context...the higher the value, the higher the likelihood. Let us now describe the though process of how these features are derived.\n",
    "\n",
    "To visualize the process, let's consider the (woman,stove,hot) tripple. We want our function to regard the two nouns as being used in a different context. For this example, let's suggest that \"woman\" belongs with the \"attractive\" definition of \"hot\", and \"stove\" belongs with the \"temperature\" definition. Now let's imagine what we might see in the definition score list. If our definition score list was ideal, we would see a high score for the temperature definition for noun2 (stove), a zero score for the temperature definition for noun1 (woman), a high score for the attractive definition for noun2, and a zero score for the attractive definition for noun2, and zeros for all other scores.\n",
    "\n",
    "Our scoring technique will be to calculate the absolute value of the difference of corresponding scores in the definition score list, and then sum these values. For example, suppose noun1 has the definition score list [0,1,0,0,0], and noun2 has the definition score list [1,0,0,0,0] (we are assuming the first element corresponds to the temperature definition and the second corresponds to the attractive definition). Then the absolute value of the difference would be the list [1,1,0,0,0], and its sum would be 2.\n",
    "\n",
    "Notice that when one definition has a high score for one noun and a low score for the other, a high value is added to the final score. On the other hand, if a definition has a non zero score for both nouns, this could suggest that both nouns fit this definition (indicating same context), or happens by chance due to the wording of the definition (and should not contribute to our certainty of different context), then the absolute value of the difference will be small, contributing to a low final score. \n",
    "\n",
    "Consider another case where each definition has a non zero score for one noun and a zero score for the other noun. In this case, our definition scoring technique is suggesting uncertainty in which definition the noun belongs to (since there are multiple non zero values for each noun), which suggests low confidence in a \"different context\" prediction. However, on the other hand, for each definition, our scoring technique is suggesting that each particular definition could belong to one noun but definitely not the other. This suggests a \"different context\" prediction. Our sum absolute difference scoring technique will score this situation incredibly high, in fact, it will return a higher score than our ideal situation where we have a non zero score for the correct definition and zeros otherwise. To counter this, we must include a \"normalization\" technique. Let's refer to this normalization correction as normalization 1.\n",
    "\n",
    "Another problem with our sum-abosolute-difference scoring technique is that adjectives with more definitions are likely to receive higher scores than adjectives with few definitions (more terms to sum). To counter this, we need another normalization technique, which we will refer to as normalization 2.\n",
    "\n",
    "Thus, we will be deriving multiple feature scores since we have seven different collocation scoring methods, and we can derive several different methods of normalizing our scores. Since we don't know, apriori what collocation scores or normalization method is the best, we should consider different combinations. Thus, we will have multiple score features.\n",
    "\n",
    "Now that we have described the thought process of deriving our features, let's describe what the function actuall does...\n",
    "\n",
    "The first thing the function does is record the length of the definitions list. This value will be used for \"normalizing\" or \"averaging\" purposes. Recall that nouns are more likely to be assigned to different definitions if the number of possible definitions is greater. Thus, we must account for this.\n",
    "\n",
    "The next thing that the function does is use the score_getter function to obtain seven different collocation scores for each definition for each noun. Lists bs1 and bs2 contain definition scores for noun1 and noun2 respectively. \n",
    "\n",
    "Recall that the definition scorer returns seven possible definition scoring techniques. We separate each scoring method into seven different lists. For noun1, we create seven different lists, I1a...I1g, where each list has definition scores using one particular scoring method. The \"I\" in I1a stands for \"intermediate\", the \"1\" refers to noun1, and the \"a\" is an index for the particular scoring method. We then do the same for noun2, with lists I2a...I2g.\n",
    "\n",
    "Next we create an intermediate score list called iscorelist, defined by iscorelist=[(I1a,I2a),(I1b,I2b),(I1c,I2c),(I1d,I2d),(I1e,I2e),(I1f,I2f),(I1g,I2g)]. Remember that features are created for noun, noun, adjective tripples. Thus, we need to compare definition score lists of noun1 and noun2 together.\n",
    "\n",
    "The next step is to normalize the scores (in several different ways). The first \"normalization\" technique is to do nothing at all...and leave the scores as is. The next method is to divide each definition score by the sum of all scores. Thus, for noun1 (and noun2), all definition scores will sum to 1.\n",
    "\n",
    "The second normalization technique will be to penalize definition scores that have multiple non zero entries (\"normalization 1\" correction). To do this we may divide each score by the sum of scores. Thus the sum of the scores will add to one. However, this may present a problem. This normalization technique equalizes the weight of the scores for the definitions of both nouns. However, one noun's definition score list may have a very high definition score for a particular definition (and low uncertainty), and this score should not be diluted. To remedy this problem, let's consider another normalization technique where we divide all definition scores (of a particular noun's definition score list) by the sum of scores, and then multiply by the maximum value. \n",
    "\n",
    "Finally, we must convert a pair of normalized definition score lists into a single number (feature value). We do this by summing the absolute value of differences. We may then include a penalty for adjectives that have many different definitions (\"normalization 2\" correction). To do this, we simply divide the feature value by the number of different definitions the adjective has. We will also consider the non normalized version of this score as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_getter(noun1,noun2,definitions):\n",
    "    l=len(definitions)\n",
    "    \n",
    "    #Intermediate scores\n",
    "    bs1=score_getter(definitions,noun1)\n",
    "    bs2=score_getter(definitions,noun2)\n",
    "    \n",
    "    I1a=pd.Series([tup[0] for tup in bs1])\n",
    "    I2a=pd.Series([tup[0] for tup in bs2])\n",
    "    \n",
    "    I1b=pd.Series([tup[1] for tup in bs1])\n",
    "    I2b=pd.Series([tup[1] for tup in bs2])\n",
    "    \n",
    "    I1c=pd.Series([tup[2] for tup in bs1])\n",
    "    I2c=pd.Series([tup[2] for tup in bs2])\n",
    "    \n",
    "    I1d=pd.Series([tup[3] for tup in bs1])\n",
    "    I2d=pd.Series([tup[3] for tup in bs2])\n",
    "    \n",
    "    I1e=pd.Series([tup[4] for tup in bs1])\n",
    "    I2e=pd.Series([tup[4] for tup in bs2])\n",
    "    \n",
    "    I1f=pd.Series([tup[5] for tup in bs1])\n",
    "    I2f=pd.Series([tup[5] for tup in bs2])\n",
    "    \n",
    "    I1g=pd.Series([tup[6] for tup in bs1])\n",
    "    I2g=pd.Series([tup[6] for tup in bs2])\n",
    "    \n",
    "    iscorelist=[(I1a,I2a),(I1b,I2b),(I1c,I2c),(I1d,I2d),(I1e,I2e),(I1f,I2f),(I1g,I2g)]\n",
    "    #Normalize these scores\n",
    "    iscorenormlist=[]\n",
    "    for tup in iscorelist:\n",
    "        i1=tup[0]\n",
    "        i2=tup[1]\n",
    "        i1sum=i1.sum()\n",
    "        i2sum=i2.sum()\n",
    "        \n",
    "        #append non normalized definition score lists\n",
    "        iscorenormlist.append((i1,i2))\n",
    "        \n",
    "        #append score lists that are normalized by their sums\n",
    "        if i1sum>0:\n",
    "            i1sumnorm=i1/i1sum\n",
    "        else:\n",
    "            i1sumnorm=i1\n",
    "        if i2sum>0:\n",
    "            i2sumnorm=i2/i2sum\n",
    "        else:\n",
    "            i2sumnorm=i2\n",
    "        iscorenormlist.append((i1sumnorm,i2sumnorm))\n",
    "        \n",
    "        #append score lists that are normalized by multiplying by the ratio of the max to the sum\n",
    "        i1max=i1.max()\n",
    "        i2max=i2.max()\n",
    "        if i1sum>0:\n",
    "            i1norm2=i1*i1max/i1sum\n",
    "        else:\n",
    "            i1norm2=i1\n",
    "        if i2sum>0:\n",
    "            i2norm2=i2*i2max/i2sum\n",
    "        else:\n",
    "            i2norm2=i2\n",
    "        iscorenormlist.append((i1norm2,i2norm2))\n",
    "        \n",
    "        finalscorelist=[]\n",
    "        for tup in iscorenormlist:\n",
    "            s1=tup[0]\n",
    "            s2=tup[1]\n",
    "            l=len(s1)\n",
    "            \n",
    "            diffsum=np.abs(s1-s2).sum()\n",
    "            finalscorelist.append(diffsum)\n",
    "            finalscorelist.append(diffsum/l)\n",
    "    return finalscorelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Models\n",
    "\n",
    "In this section, we will train our predictive models. We begin by computing feature values for our samples.\n",
    "\n",
    "Recall that our trainset database has a column of samples, which are noun, noun, adjective tripples, and a target column, where 1 indicates the nouns are used in a different context with the adjective, and a 0 indicates same context. Also recall that our feature_getter function returns a list of features when one sample is inputed. To create our training dataset, we will create a list of features, then transform the list into matrix format.\n",
    "\n",
    "Computing the features is computationally expensive (it takes a long time). However, this process can be done in parallel. Thus, I will compute 1/6 of the samples on each of my six cpu cores to bring the processing time down from 3hr 20 min to about 32 min.\n",
    "\n",
    "Having already created the dataframe, as mentioned above, we may now load the feature dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our dataset\n",
    "handle=open(\"/home/mike/databases/wsd/traindf2\",\"rb\")\n",
    "traindf=pickle.load(handle)\n",
    "handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=traindf.copy()\n",
    "del X[\"sample\"]\n",
    "del X[\"target\"]\n",
    "y=traindf.target\n",
    "\n",
    "Xtrainval,Xtest,ytrainval,ytest=train_test_split(X,y,random_state=0,test_size=0.15)\n",
    "Xtrain,Xval,ytrain,yval=train_test_split(Xtrainval,ytrainval,random_state=0,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 4159\n",
      "Val length: 735\n",
      "Test length: 864\n"
     ]
    }
   ],
   "source": [
    "#Examine the lengths of the splits\n",
    "print(\"Train length: {}\\nVal length: {}\\nTest length: {}\".format(len(Xtrain),len(Xval),len(Xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model\n",
    "\n",
    "Train, validate and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.18 s, sys: 0 ns, total: 8.18 s\n",
      "Wall time: 8.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#max depths for the trees\n",
    "depths=[2,5,10,20,40,80,160,320,640]\n",
    "\n",
    "#Create lists of fitted random forest classifiers and their train and val scores\n",
    "models=[RandomForestClassifier(n_estimators=100,max_depth=d,random_state=123).fit(Xtrain,ytrain) for d in depths]\n",
    "trainscores=[model.score(Xtrain,ytrain) for model in models]\n",
    "valscores=[model.score(Xval,yval) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa2fb516438>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFhlJREFUeJzt3X+MXedd5/H3p5NO06a/XDxUxT9id+VAsyAl9MplyKod1SRxQ1UXkJCD6KYIYZBIBCmwSlbVhnUWuX9UC/uHVTCtaQo0Vjb80GgVbRqcjAp0UuY6SVvs4NRxoZ5JdzPUCd3uRriefvaPcyY9mTi+99p35ozn+bykq3PPc55z7/da9uceP+fc58g2ERFRhle1XUBERKychH5EREES+hERBUnoR0QUJKEfEVGQhH5EREES+hERBUnoR0QUJKEfEVGQy9ouYKn169d7y5YtbZcREXFJOXLkyD/bHuvVr2foSzoIvB941vYPn2O7gP8G3AT8P+DDth+rt90CfLTu+l9s39Pr/bZs2UK32+3VLSIiGiT9Uz/9+hne+TSw8zzb3wdsqx97gE/UBbwFuAt4F7AduEvSun6KioiI5dEz9G1/Hjh9ni67gM+48ijwZklvA24EHrJ92vZzwEOc/8sjIiKW2TBO5G4ATjXWZ+u2V2qPiIiWrIqrdyTtkdSV1J2fn2+7nIiINWsYoT8HbGqsb6zbXqn9ZWwfsN2x3Rkb63nyOSIiLtAwQn8S+Peq/BjwL7a/ATwI3CBpXX0C94a6LSIiWtLPJZv3AhPAekmzVFfkvBrA9u8DD1BdrnmC6pLNX6i3nZZ0NzBTv9Re2+c7IRxRmZ6GqSmYmIDx8bar+Z7UNZjUNZiVqsv2qnq8853vdBTsC1+wX/tae2SkWn7hC21XVEldg0ldgxlCXUDXfWTsqjiRG/GiqSk4cwYWFqrl1FTbFVVS12BS12BWsK6EfqwuExMwOgojI9VyYqLtiiqpazCpazArWJeq/xWsHp1Ox5mGoXClj7kOKnUNZo3WJemI7U7Pfgn9iIhLX7+hn+GdiIiCJPQjIgqS0I+IKEhCPyKiIAn9iIiCJPQjIgqS0I+IKEhCPyKiIAn9iIiCJPQjIgqS0I+IKEhCv2TT07BvX7WMiCL0vHNWrFHT07BjRzV39+goHD68umYcjIhlkSP9Uq3Wm0lExLLqK/Ql7ZR0XNIJSXecY/uVkg5L+rKkKUkbG9sWJD1RPyaHWXxchNV6M4mIWFb93Bh9BNgPXA/MAjOSJm0fa3T7OPAZ2/dIei+wD/hQve0F29cMue64WOPj1ZDOaryZREQsm37G9LcDJ2yfBJB0CNgFNEP/auAj9fNHgL8cZpGxTMbHE/YRhelneGcDcKqxPlu3NX0J+On6+U8Bb5D0ffX65ZK6kh6V9MFzvYGkPXWf7vz8/ADlR0TEIIZ1Ivc3gfdIehx4DzAHLNTbrqxv4fVzwO9J+jdLd7Z9wHbHdmdsbGxIJUVExFL9DO/MAZsa6xvrthfZfob6SF/S64Gfsf18vW2uXp6UNAVcCzx90ZVHRMTA+jnSnwG2SdoqaRTYDbzkKhxJ6yUtvtadwMG6fZ2k1yz2Aa7jpecCIiJiBfUMfdtngVuBB4EngftsH5W0V9IH6m4TwHFJTwFvBX6nbn8H0JX0JaoTvB9bctVPRESsINluu4aX6HQ67na7bZcREXFJkXSkPn96XvlFbkREQRL6EREFSehHRBQkoR8RUZCEfkREQRL6EREFSehHRBQkoR8RUZCEfkREQRL6EREFSehHRBQkoR8RUZCEfkREQRL6EREFSehHRBQkoR8RUZCEfkREQfoKfUk7JR2XdELSHefYfqWkw5K+LGlK0sbGtlskfbV+3DLM4mNtmp6GffuqZUQM12W9OkgaAfYD1wOzwIykySX3uv048Bnb90h6L7AP+JCktwB3AR3AwJF63+eG/UFibZiehh074MwZGB2Fw4dhfLztqiLWjn6O9LcDJ2yftH0GOATsWtLnauDh+vkjje03Ag/ZPl0H/UPAzosvO9aqqakq8BcWquXUVNsVRawt/YT+BuBUY322bmv6EvDT9fOfAt4g6fv63DfiRRMT1RH+yEi1nJhou6KItWVYJ3J/E3iPpMeB9wBzwEK/O0vaI6krqTs/Pz+kkuJSND5eDencfffqG9pZrecaUtdgSq+r55g+VYBvaqxvrNteZPsZ6iN9Sa8Hfsb285LmgIkl+04tfQPbB4ADAJ1Ox/2XH2vR+PjqCntYvecaUlfqGlQ/R/ozwDZJWyWNAruByWYHSeslLb7WncDB+vmDwA2S1klaB9xQt0VcUlbruYbUNZjU1Ufo2z4L3EoV1k8C99k+KmmvpA/U3SaA45KeAt4K/E6972ngbqovjhlgb90WcUlZrecaUtdgUhfIXl2jKZ1Ox91ut+0yIl5mero6ApuYWB1DAotS12DWal2Sjtju9OyX0I+IuPT1G/qZhiEioiAJ/YiIgiT0IyIKktCPiChIQj8ioiAJ/YiIgiT0IyIKktCPiChIQj8ioiAJ/YiIgiT0IyIKktCPiChIQj8ioiAJ/YiIgiT0IyIKktCPiChIQj8ioiB9hb6knZKOSzoh6Y5zbN8s6RFJj0v6sqSb6vYtkl6Q9ET9+P1hf4CIiOjfZb06SBoB9gPXA7PAjKRJ28ca3T5KdcP0T0i6GngA2FJve9r2NcMtOyIiLkQ/R/rbgRO2T9o+AxwCdi3pY+CN9fM3Ac8Mr8RYLtPTsG9ftYyIMvQ80gc2AKca67PAu5b0+W3gc5JuA64AfqKxbaukx4FvAR+1/ddL30DSHmAPwObNm/suPi7c9DTs2AFnzsDoKBw+DOPjbVcVEcttWCdybwY+bXsjcBPwx5JeBXwD2Gz7WuAjwGclvXHpzrYP2O7Y7oyNjQ2ppDifqakq8BcWquXUVNsVRcRK6Cf054BNjfWNdVvTLwL3AdieBi4H1tv+V9vfrNuPAE8DV11s0XHxJiaqI/yRkWo5MdF2RRGxEvoJ/Rlgm6StkkaB3cDkkj5fB3YASHoHVejPSxqrTwQj6e3ANuDksIqPCzc+Xg3p3H13hnYiStJzTN/2WUm3Ag8CI8BB20cl7QW6tieB3wD+UNLtVCd1P2zbkt4N7JX0HeC7wK/YPr1snyYGMj6esI8ojWy3XcNLdDodd7vdtsuIiLikSDpiu9OrX36RGxFRkIR+RERBEvoREQVJ6EdEFCShHxFRkIR+RERBEvoREQVJ6EdEFCShHxFRkIR+RERBEvoREQVJ6EdEFCShHxFRkIR+RERBEvoREQVJ6EdEFCShHxFRkL5CX9JOScclnZB0xzm2b5b0iKTHJX1Z0k2NbXfW+x2XdOMwi4+IiMH0vEdufWPz/cD1wCwwI2nS9rFGt48C99n+hKSrgQeALfXz3cC/BX4A+CtJV9leGPYHiYiI3vo50t8OnLB90vYZ4BCwa0kfA2+sn78JeKZ+vgs4ZPtfbX8NOFG/XkREtKCf0N8AnGqsz9ZtTb8N/LykWaqj/NsG2DciIlbIsE7k3gx82vZG4CbgjyX1/dqS9kjqSurOz88PqaSIiFiqn2CeAzY11jfWbU2/CNwHYHsauBxY3+e+2D5gu2O7MzY21n/1ERExkH5CfwbYJmmrpFGqE7OTS/p8HdgBIOkdVKE/X/fbLek1krYC24C/G1bxERExmJ5X79g+K+lW4EFgBDho+6ikvUDX9iTwG8AfSrqd6qTuh20bOCrpPuAYcBb41Vy5ExHRHlXZvHp0Oh13u922y4iIuKRIOmK706tffpEbEVGQhH5EREES+hERBUnoR0QUJKEfEVGQhH5EREES+hERBUnoR0QUJKEfEVGQhH5EREES+hERBUnoR0QUJKEfEVGQhH5EREES+hERBUnoR0QUJKEfEVGQvkJf0k5JxyWdkHTHObb/rqQn6sdTkp5vbFtobFt6b92IiFhBPe+RK2kE2A9cD8wCM5ImbR9b7GP79kb/24BrGy/xgu1rhldyRERcqH6O9LcDJ2yftH0GOATsOk//m4F7h1FcREQMVz+hvwE41VifrdteRtKVwFbg4Ubz5ZK6kh6V9MELrjQiIi5az+GdAe0G7re90Gi70vacpLcDD0v6iu2nmztJ2gPsAdi8efOQS4qIiEX9HOnPAZsa6xvrtnPZzZKhHdtz9fIkMMVLx/sX+xyw3bHdGRsb66OkiIi4EP2E/gywTdJWSaNUwf6yq3Ak/RCwDphutK2T9Jr6+XrgOuDY0n0jImJl9BzesX1W0q3Ag8AIcND2UUl7ga7txS+A3cAh227s/g7gDyR9l+oL5mPNq34iImJl6aUZ3b5Op+Nut9t2GRERlxRJR2x3evXLL3IjIgqS0I+IKEhCfyVMT8O+fdUyIqJFw75OP5aanoYdO+DMGRgdhcOHYXy87aoiolA50l9uU1NV4C8sVMupqbYrioiCJfSX28REdYQ/MlItJybarigiCpbhneU2Pl4N6UxNVYGfoZ2IaFFCfyWMjyfsI2JVyPBORERBEvoREQVJ6EdEFCShHxFRkIR+RERBEvoREQVJ6EdEFCShHxFRkIR+RERBEvoREQXpK/Ql7ZR0XNIJSXecY/vvSnqifjwl6fnGtlskfbV+3DLM4iMiYjA9596RNALsB64HZoEZSZPNG5zbvr3R/zbg2vr5W4C7gA5g4Ei973ND/RQREdGXfo70twMnbJ+0fQY4BOw6T/+bgXvr5zcCD9k+XQf9Q8DOiyk4IiIuXD+hvwE41VifrdteRtKVwFbg4UH3jYiI5TfsE7m7gfttLwyyk6Q9krqSuvPz80MuKSIiFvUT+nPApsb6xrrtXHbzvaGdvve1fcB2x3ZnbGysj5IiIuJC9BP6M8A2SVsljVIF++TSTpJ+CFgHTDeaHwRukLRO0jrghrotIiJa0PPqHdtnJd1KFdYjwEHbRyXtBbq2F78AdgOHbLux72lJd1N9cQDstX16uB8hIiL6pUZGrwqdTsfdbrftMiIiLimSjtju9OqXX+RGRBQkoR8RUZCEfkREQRL6EREFSehHRBQkoR8RUZCEfkREQRL6EREFSehHRBQkob8Cpqdh375qGRHRpp5z78TFmZ6GHTvgzBkYHYXDh2F8vO2qIqJUOdJfZlNTVeAvLFTLqam2K4qIkiX0l9nERHWEPzJSLScm2q4oIkqW4Z1lNj5eDelMTVWBn6GdiGhTQn8FjI8n7CNidcjwTkREQRL6EREF6Sv0Je2UdFzSCUl3vEKfn5V0TNJRSZ9ttC9IeqJ+vOzeuhERsXJ6julLGgH2A9cDs8CMpEnbxxp9tgF3AtfZfk7S9zde4gXb1wy57oiIuAD9HOlvB07YPmn7DHAI2LWkzy8B+20/B2D72eGWGRERw9BP6G8ATjXWZ+u2pquAqyT9raRHJe1sbLtcUrdu/+BF1nteme4gIuL8hnXJ5mXANmAC2Ah8XtKP2H4euNL2nKS3Aw9L+ortp5s7S9oD7AHYvHnzBRWQ6Q4iInrr50h/DtjUWN9YtzXNApO2v2P7a8BTVF8C2J6rlyeBKeDapW9g+4Dtju3O2NjYwB8CMt1BREQ/+gn9GWCbpK2SRoHdwNKrcP6S6igfSeuphntOSlon6TWN9uuAYyyDTHcQEdFbz+Ed22cl3Qo8CIwAB20flbQX6NqerLfdIOkYsAD8lu1vSvpx4A8kfZfqC+Zjzat+hinTHURE9CbbbdfwEp1Ox91ut+0yIiIuKZKO2O706pdf5EZEFCShHxFRkIR+RERBEvoREQVJ6EdEFCShHxFRkIR+RERBEvoREQVJ6EdEFCShHxFRkIR+RERB1lbo5y4qERHnNaybqLQvd1GJiOhp7Rzp5y4qERE9rZ3Qz11UIiJ6WjvDO7mLSkRET2sn9KEK+oR9RMQrWjvDOxER0VNfoS9pp6Tjkk5IuuMV+vyspGOSjkr6bKP9FklfrR+3DKvwiIgYXM/hHUkjwH7gemAWmJE02bzBuaRtwJ3Adbafk/T9dftbgLuADmDgSL3vc8P/KBER0Us/R/rbgRO2T9o+AxwCdi3p80vA/sUwt/1s3X4j8JDt0/W2h4Cdwyk9IiIG1U/obwBONdZn67amq4CrJP2tpEcl7Rxg34iIWCHDunrnMmAbMAFsBD4v6Uf63VnSHmAPwObNm4dUUkRELNVP6M8BmxrrG+u2plngi7a/A3xN0lNUXwJzVF8EzX2nlr6B7QPAAQBJ85L+qc/6z2U98M8Xsf9ySV2DSV2DSV2DWYt1XdlPJ9k+fwfpMuApYAdViM8AP2f7aKPPTuBm27dIWg88DlxDffIW+NG662PAO22fHuyz9E9S13ZnuV7/QqWuwaSuwaSuwZRcV88jfdtnJd0KPAiMAAdtH5W0F+janqy33SDpGLAA/JbtbwJIupvqiwJg73IGfkREnF9fY/q2HwAeWNL2nxrPDXykfizd9yBw8OLKjIiIYViLv8g90HYBryB1DSZ1DSZ1DabYunqO6UdExNqxFo/0IyLiFayZ0O9nfqA2SDoo6VlJf992LYskbZL0SGOupF9ruyYASZdL+jtJX6rr+s9t19QkaUTS45L+R9u1NEn6R0lfkfSEpG7b9SyS9GZJ90v6B0lPSmp9ClxJP1j/OS0+viXp19uuC0DS7fXf+7+XdK+ky5flfdbC8E49P9BTNOYHorqE9Nh5d1wBkt4NfBv4jO0fbrseAElvA95m+zFJb6C6rPaDbf95SRJwhe1vS3o18DfAr9l+tM26Fkn6CNU8Um+0/f6261kk6R+Bju1Vdd25pHuAv7b9SUmjwOtsP992XYvq3JgD3mX7Yn4bNIxaNlD9fb/a9guS7gMesP3pYb/XWjnS72d+oFbY/jywqi5Ttf0N24/Vz/8P8CSrYHoMV75dr766fqyKoxJJG4GfBD7Zdi2XAklvAt4NfArA9pnVFPi1HcDTbQd+w2XAa+vfRr0OeGY53mSthH7m+LlAkrYA1wJfbLeSSj2E8gTwLNVkfauiLuD3gP8AfLftQs7BwOckHamnNFkNtgLzwB/VQ2KflHRF20UtsRu4t+0iAGzPAR8Hvg58A/gX259bjvdaK6EfF0DS64E/A37d9rfargfA9oLta6im7NguqfUhMUnvB561faTtWl7Bv7P9o8D7gF+thxTbdhnVL/E/Yfta4P8Cq+lc2yjwAeC/t10LgKR1VKMTW4EfAK6Q9PPL8V5rJfT7mR8oGuox8z8D/tT2n7ddz1L1UMAjrI6puK8DPlCPnR8C3ivpT9ot6Xvqo8TFKc3/gmq4s22zwGzjf2r3873pWFaD9wGP2f7fbRdS+wnga7bn6znM/hz48eV4o7US+jPANklb62/w3cBkyzWtWvUJ008BT9r+r23Xs0jSmKQ3189fS3Vi/h/arQps32l7o+0tVH+3Hra9LEdhg5J0RX0ynnr45Aag9SvFbP8v4JSkH6ybdgCtX1jRcDOrZGin9nXgxyS9rv73uYPqXNvQrYkbo7/S/EAtlwWApHupZhpdL2kWuMv2p9qtiuuADwFfqcfPAf5jPd1Gm94G3FNfVfEq4D7bq+ryyFXorcBfVDnBZcBnbf/Pdkt60W3An9YHYieBX2i5HuDFL8frgV9uu5ZFtr8o6X6qSSnPUk1auSy/zl0Tl2xGRER/1srwTkRE9CGhHxFRkIR+RERBEvoREQVJ6EdEFCShHxFRkIR+RERBEvoREQX5/8rRRjLUG4suAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot train and val scores\n",
    "plt.plot(range(len(depths)),trainscores,\".\",color=\"red\")\n",
    "plt.plot(range(len(depths)),valscores,\".\",color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.9034013605442177\n",
      "Best depth: 20\n"
     ]
    }
   ],
   "source": [
    "#Best validation score\n",
    "print(\"Best accuracy: {}\\nBest depth: {}\".format(max(valscores),depths[valscores.index(max(valscores))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel=RandomForestClassifier(n_estimators=100,max_depth=20).fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.989660976196201\n",
      "Validation Accuracy: 0.8993197278911564\n",
      "Test Accuracy: 0.9155092592592593\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy: {}\\nValidation Accuracy: {}\\nTest Accuracy: {}\".format(rfmodel.score(Xtrain,ytrain),\n",
    "                                                                             rfmodel.score(Xval,yval),\n",
    "                                                                             rfmodel.score(Xtest,ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain this model on the trainval set and test on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel2=RandomForestClassifier(n_estimators=200,max_depth=20).fit(Xtrainval,ytrainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9760931753167144\n",
      "Test Accuracy: 0.9155092592592593\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy: {}\\nTest Accuracy: {}\".format(rfmodel.score(Xtrainval,ytrainval),rfmodel.score(Xtest,ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
